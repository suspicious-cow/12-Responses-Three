{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Completions Three \n",
    "\n",
    "## Basic Connection and Packages\n",
    "\n",
    "### Importing OpenAI and Initializing the Client\n",
    "\n",
    "To begin, we'll import the `OpenAI` class from the `openai` library, which allows us to interact with the OpenAI API. Next, we initialize a client instance, which we'll use to send requests and receive responses from the OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is a simple example of using the OpenAI API\n",
    "It uses the OpenAI Python client library to open a connection to the OpenAI API.\n",
    "This also looks for the OPENAI_API_KEY environment variable to authenticate the client.\n",
    "\"\"\"\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Options\n",
    "\n",
    "### Real-Time Responses Using the Stream Parameter\n",
    "\n",
    "In this example, we introduce the `stream` parameter (`stream=True`) to enable real-time streaming of the model's output. Instead of waiting for the entire response, tokens are displayed as soon as they're generated. This approach is particularly useful for interactive applications, chatbots, or scenarios where immediate feedback enhances user engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the stream parameter to dynamically show tokens to the user in real-time.\n",
    "In this case, we want the response to start showing as soon as possible.\n",
    "\"\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=True \n",
    "    )\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Token Usage Statistics with `stream_options`\n",
    "\n",
    "In this example, we introduce the `stream_options` parameter to access detailed token usage statistics during streaming responses from the OpenAI API. Setting `stream_options={\"include_usage\": True}` enables real-time reporting of token usage, providing insights into:\n",
    "\n",
    "- **Prompt Tokens**: Number of tokens used in your prompt.\n",
    "- **Completion Tokens**: Number of tokens generated by the model in response.\n",
    "- **Total Tokens**: Combined total of prompt and completion tokens.\n",
    "\n",
    "This approach is helpful for monitoring and managing token consumption during model interactions, especially when optimizing costs or tracking usage limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the stream_options parameter to show token usage statistics.\n",
    "\"\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write one paragraph about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True},\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    # Handle content chunks\n",
    "    if hasattr(chunk, 'choices') and len(chunk.choices) > 0:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "    # Handle usage or other non-content chunks\n",
    "    elif hasattr(chunk, 'usage') and chunk.usage is not None:\n",
    "        print(\"\\n\\nUsage Statistics:\")\n",
    "        print(f\"Prompt Tokens: {chunk.usage.prompt_tokens}\")\n",
    "        print(f\"Completion Tokens: {chunk.usage.completion_tokens}\")\n",
    "        print(f\"Total Tokens: {chunk.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Raw Stream Chunks for Debugging\n",
    "\n",
    "In this example, we print the raw data (`chunk`) returned from the streaming response. Inspecting raw chunks can help you better understand the structure of the streamed responses from the OpenAI API. This approach is particularly valuable for debugging or exploring how streaming data is structured, especially when you’re handling different kinds of chunks—such as content chunks or chunks containing usage statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write one paragraph about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True},\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    # Print the raw chunk data for inspection\n",
    "    print(\"\\nRaw Chunk:\", chunk)\n",
    "    \n",
    "    # Handle content chunks\n",
    "    if hasattr(chunk, 'choices') and len(chunk.choices) > 0:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "    # Handle usage or other non-content chunks\n",
    "    elif hasattr(chunk, 'usage') and chunk.usage is not None:\n",
    "        print(\"\\n\\nUsage Statistics:\")\n",
    "        print(f\"Prompt Tokens: {chunk.usage.prompt_tokens}\")\n",
    "        print(f\"Completion Tokens: {chunk.usage.completion_tokens}\")\n",
    "        print(f\"Total Tokens: {chunk.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modalities\n",
    "\n",
    "### Specifying Output Modality with the `modalities` Parameter\n",
    "\n",
    "In this example, we introduce the `modalities` parameter to explicitly define the type of content the OpenAI API returns. By setting `modalities=[\"text\"]`, we specify that the model's output should be purely text-based. This parameter is useful when interacting with multimodal models capable of producing various output types (text, images, audio, etc.), allowing precise control over the desired output format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the modalities parameter to set the type of output we receive.\n",
    "In this case, we want the response to be text-based.\n",
    "\"\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=True,\n",
    "    modalities=[\"text\"], \n",
    "    )\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n\n",
    "\n",
    "### Generating Multiple Completions Using the `n` Parameter\n",
    "\n",
    "In this example, we introduce the `n` parameter, which specifies the number of distinct completions to generate from a single prompt. By setting `n=3`, we request three separate responses from the model for the same input. This approach is valuable for generating diverse ideas, exploring multiple writing styles, or choosing from alternative outputs.\n",
    "\n",
    "The maximum possible value for `n` is `128` completions\n",
    "\n",
    "The script collects each of the three completions individually from the stream and prints them separately for easy comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the n parameter to set the number of completions we receive.\n",
    "\"\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=True,\n",
    "    modalities=None,\n",
    "    n=3,  # Number of completions to generate \n",
    ")\n",
    "\n",
    "# Dictionary to store the content of each completion\n",
    "completions = {0: \"\", 1: \"\", 2: \"\"}\n",
    "\n",
    "# Iterate over the stream chunks\n",
    "for chunk in stream:\n",
    "    # Loop through all choices in the current chunk\n",
    "    for choice in chunk.choices:\n",
    "        if choice.delta.content is not None:\n",
    "            # Append the content to the corresponding completion\n",
    "            completions[choice.index] += choice.delta.content\n",
    "\n",
    "# Print all three completions\n",
    "for i in range(3):\n",
    "    print(f\"\\nCompletion {i + 1}:\\n{completions[i]}\\n{'-' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can We Stream All Three Simultaneously?\n",
    "\n",
    "When `stream=True` and `n>1`, the OpenAI API doesn't guarantee simultaneous, perfectly synchronized streaming of all completions. Instead:\n",
    "\n",
    "- **Each chunk** may contain updates for one or more of the completions, depending on how the model generates them.\n",
    "- **Chunks arrive sequentially**, and each chunk's choices reflect the progress of each completion at that moment.\n",
    "- **Real-time parallel streaming**—such as displaying multiple completions side-by-side as they grow—is not possible, as the API streams completions as a single sequential event stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Multiple Completions without Streaming\n",
    "\n",
    "In this example, we use the `n` parameter along with `stream=False` to generate multiple completions simultaneously without streaming. By setting `n=3`, the API returns three distinct completions in a single response, each accessible individually. This method simplifies gathering multiple responses, making it straightforward to review, compare, and select from multiple generated outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the n parameter to set the number of completions we receive.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write one sentence about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=False,  # Disable streaming\n",
    "    n=10,  # Number of completions to generate \n",
    ")\n",
    "\n",
    "# Print all three completions\n",
    "for i, choice in enumerate(response.choices):\n",
    "    print(f\"\\nCompletion {i + 1}:\\n{choice.message.content}\\n{'-' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding User, Store, and Metadata Parameters for Enhanced Tracking\n",
    "\n",
    "In this example, we use additional parameters—`user`, `store`, and `metadata`—with the OpenAI API. These parameters provide ways to track and organize your API interactions:\n",
    "\n",
    "- **`user`**: Assigns an identifier (`\"user_id_123\"`) to associate completions with a specific user.\n",
    "- **`store`**: Indicates (`True`) that the API should retain the interaction, allowing later retrieval or analysis.\n",
    "- **`metadata`**: Adds custom structured data (such as project details, audience age, priority, and genre) to help categorize or filter interactions within your application or analytics pipeline.\n",
    "\n",
    "**Note:** The `store` parameter must be set to `True` in order to use the `metadata` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the user, store, and metadata parameters to set values that we can use later.\n",
    "\"\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=True,\n",
    "    modalities=None, \n",
    "    user=\"user_id_123\",\n",
    "    store=True,\n",
    "    metadata={\n",
    "        \"project\": \"Frog Adventures Book Series\",\n",
    "        \"audience_age\": \"5-8\",\n",
    "        \"priority\": \"medium\",\n",
    "        \"genre\": \"educational fiction\"\n",
    "    },\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_api_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
